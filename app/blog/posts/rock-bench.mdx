---
title: "That's not The Rock"
date: "November 2025"
summary: "Testing  the top image editing models across 100 recursive Dwaynes."
---

I regularly work with image editing models in my day-to-day. They're fantastic, but they generally fall apart if you edit the edit of the edit's edit.

Earlier this year, [u/Foreign_Builder_2238](https://www.reddit.com/u/Foreign_Builder_2238) demonstrated this by asking ChatGPT to endlessly recreate a photo of Dwayne 'The Rock' Johnson, instructing it to "Create an exact replica of this image, don't change a thing." I decided to put the modern image editors through that same recursive abuse (Nano Banana Pro, SeeDream 4, Qwen, and friends) to see how they unravel when looping their own outputs 100 times.

The findings were both interesting and ridiculous, with all the models tested behaving in slightly different ways. For a measure I calculated the structural similarity index for each generated image compared with the original. These are graphed for each model below. Note that using this for a measure has some flaws, minor differences in Dwayne's position will destroy the SSIM score even when the image is otherwise coherent. 

The SSIM trend is interesting, but what matters more, I think, is how many recursions it takes before the subject simply isn't The Rock anymore. So, I’ve tracked the "That's Not The Rock" (TNTR) score for each model—the exact generation where Dwayne ceases to be Dwayne. Since this is subjective, I've also included a way for you to cast your own vote on when he loses his Rock-ness.

*Note: Most models were only tested once, it is possible that we'd see entirely different results on retesting.* 


<h2 id="gpt-image1">GPT-Image1</h2>

The original reddit post was 7 months prior to writing this; let's check in on how things have changed.

<LocalVideo
  src="/videos/gpt_evolution.mp4?v=2"
  caption="Quite disappointing, really. Recursive generations almost immediately degrade to static, only slightly recovering before falling to pieces. This was the worst performing model in the test and the same behaviour was seen in multiple runs."
/>

<ModelChart
  activeModel="gpt"
  revealIndex={0}
  order={["gpt", "gptMini", "nanoBananaPro", "seedream", "qwen", "nanoBanana", "flux"]}
/>
<RockVotePrompt model="gpt" />

<h2 id="gpt-image1-mini">GPT-Image1-mini</h2>

Fortunately, the results for GPT-Image1-Mini were a lot more interesting.

<LocalVideo
  src="/videos/gptmini_evolution.mp4?v=4"
  caption="The only model that continued to create coherent images without degrading into noise. Mini clearly isn't just 'smaller' and there are more substantial differences in the models. While this had the lowest peak SSIM and TNTR scores, it was definitely the most fun."
/>

<ModelChart
  activeModel="gptMini"
  revealIndex={1}
  order={["gpt", "gptMini", "nanoBananaPro", "seedream", "qwen", "nanoBanana", "flux"]}
/>
<RockVotePrompt model="gptMini" />

<h2 id="nano-banana-pro">Nano Banana Pro</h2>

Currently touted as the SOTA of image editing models, how does it hold up?

<LocalVideo
  src="/videos/nano_banana_pro_evolution.mp4?v=2"
  caption="Far more structurally consistent, differences in colour become more exaggerated over generations. Dwayne is quite noisy by step 10 and devolves from there into fractals and eventually into something reminiscent of a broken LCD panel."
/>

<ModelChart
  activeModel="nanoBananaPro"
  revealIndex={2}
  order={["gpt", "gptMini", "nanoBananaPro", "seedream", "qwen", "nanoBanana", "flux"]}
/>
<RockVotePrompt model="nanoBananaPro" />

<h2 id="seedream-4">SeeDream 4</h2>

Currently the runner-up to Nano Banana Pro in the leaderboards, the pattern here trends to red, with several sudden jumps in image coherence.

<LocalVideo
  src="/videos/seedream_evolution.mp4?v=2"
  caption="The only model tested that got… hairy?"
/>

<ModelChart
  activeModel="seedream"
  revealIndex={3}
  order={["gpt", "gptMini", "nanoBananaPro", "seedream", "qwen", "nanoBanana", "flux"]}
  caption="The drift to the right at image 4 destroys the SSIM score early on, making the chart less useful in this case."
/>
<RockVotePrompt model="seedream" />

<h2 id="qwen-image-edit">Qwen Image Edit</h2>

A highly trainable model which has recently been gaining in popularity on the release of a few nice fine tunes. Note: On writing I noticed that the test here uses a previous generation of Qwen Image Edit, the more recent Qwen Image Edit 2509 is yet to be tested. 

<LocalVideo
  src="/videos/qwen_evolution.mp4?v=2"
  caption="The best performing model according to the numbers, with the highest peak SSIM, and a very smooth descent into blotchy green noise, distinguished only by suddenly refreshed facial hair and lips."
/>

<ModelChart
  activeModel="qwen"
  revealIndex={4}
  order={["gpt", "gptMini", "nanoBananaPro", "seedream", "qwen", "nanoBanana", "flux"]}
/>
<RockVotePrompt model="qwen" />

<h2 id="nano-banana">Nano Banana</h2>

The previous generation of the Nano Banana model behaves quite differently to the latest version. 

<LocalVideo
  src="/videos/nanobana_evolution.mp4?v=2"
  caption="The model with the most significant Dwaynian motion, with The Rock gradually sliding to the right."
/>

<ModelChart
  activeModel="nanoBanana"
  revealIndex={5}
  order={["gpt", "gptMini", "nanoBananaPro", "seedream", "qwen", "nanoBanana", "flux"]}
  caption="The sideways motion breaks the SSIM quite quickly."
/>
<RockVotePrompt model="nanoBanana" />

<h2 id="flux-kontext-pro">Flux Kontext Pro</h2>

The model that proved the usefulness of editing models; it is over a year old now.

<LocalVideo
  src="/videos/flux_evolution.mp4?v=2"
  caption="Pretty good! It performs the best of the models tested from an average SSIM perspective, but the results still trend to noise and abrupt white dudes."
/>

<ModelChart
  activeModel="flux"
  revealIndex={6}
  order={["gpt", "gptMini", "nanoBananaPro", "seedream", "qwen", "nanoBanana", "flux"]}
/>
<RockVotePrompt model="flux" />

<h2 id="all-models-compared">All Models Compared</h2>

<MegaChart />

<h3 id="model-summary">Results breakdown</h3>

<RockVoteTable />

<h2 id="other-findings">Other findings</h2>

#### GPT-image1-mini is consistently weird

I was most interested in GPT-image1-mini, since this was the only model that produced consistently coherent images rather than degrading to static. I ran this model five times to see how the variations introduced during the process affected the final result. The final frames were surprisingly similar, with only one run failing to produce a variation of a demonic white dude.

<GptMiniGallery />

#### Image coherence jumps

While most models trend to noise, several of them showed sudden jumps in image coherence. Seedream 4 was particularly prone to this, showing 4 distinct coherence jumps. 

<SuddenJumpsGallery />

<h2 id="so-what-did-we-learn">So what did I learn?</h2>

I never really set out to learn anything with this; this was only ever a fun experiment. It also isn't very surprising that there is loss in recursive generations with image editing models; it is expected that there will be loss in asking a generative model to reproduce an image, and that delta will compound over time. It is interesting, though, the breadth of variation in how these models degrade. 

If there’s a takeaway, it's to know that none of the frontier image editing models are safe from loss in recursive image editing. So beware of editing the edit of the edit; just write a better prompt the first time. 

*I'd love to know more about why these models behave so differently. If you can elighten me, send me a DM.*